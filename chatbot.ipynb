{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ab78ac",
   "metadata": {},
   "source": [
    "\n",
    "# Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f518e4",
   "metadata": {},
   "source": [
    "\n",
    "### Komponensek\n",
    "- **Konfiguráció**\n",
    "- **Dokumentumok betöltése és chunkolása**\n",
    "- **Embedding és vektortár**\n",
    "- **LLM inicializálás**\n",
    "- **RAG keresés és kontextus összeállítás**\n",
    "- **Agent workflow**\n",
    "- **Demó futtatás**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515572a1",
   "metadata": {},
   "source": [
    "## Konfiguráció\n",
    "\n",
    "Ez tartalmazza a különböző alapbeállításokat.  \n",
    "\n",
    "### Paraméterek\n",
    "- **`llm_model_name`**: `google/flan-t5-base` – nyílt LLM, demóra elég (pontosabb: `flan-t5-large`)  \n",
    "- **`temperature`** – 0.0 -> determinisztikus válaszok\n",
    "- **`embedding_model_name`**: `all-MiniLM-L6-v2` – gyors embedding model \n",
    "- **`chunk_size, chunk_overlap`** – chunk és átfedés méret\n",
    "- **`top_k`** – ennyi releváns dokumentumot ad vissza a retriever\n",
    "- **`max_iterations`** – agent ciklus ennyiszer futhat maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49d46500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    llm_model_name: str = \"google/flan-t5-base\"\n",
    "    temperature: float = 0.0\n",
    "    embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    data_dir: str = \"data\"\n",
    "    chunk_size: int = 500\n",
    "    chunk_overlap: int = 70\n",
    "    top_k: int = 4\n",
    "    max_iterations: int = 2\n",
    "\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3470df",
   "metadata": {},
   "source": [
    "## Dokumentumok betöltése és chunkolása\n",
    "\n",
    "Ebben a dokumentumokat olvassuk be és daraboljuk fel azokat.\n",
    "\n",
    "- **`load_pdf_text`**: oldalanként olvassa be a PDF-ek szövegét a `pypdf` segítségével\n",
    "- **`build_documents`**: minden fájlt `Document` objektummá alakít, a `metadata[\"source\"]` mezőben megőrizve a fájlnevet\n",
    "- **`chunk_documents`**: `RecursiveCharacterTextSplitter`-rel darabolja a szövegeket, a chunkolás biztosítja, hogy a modell releváns, kezelhető méretű kontextust kapjon\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5682455e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 221,\n",
       " [{'source': 'attention_is_all_you_need.pdf'},\n",
       "  {'source': 'The Secret, Magical Life Of Lithium - NOEMA.pdf'},\n",
       "  {'source': 'We should fix climate change — but we should not regret it - ABC Religion & Ethics.pdf'}])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pypdf import PdfReader\n",
    "from langchain.schema import Document\n",
    "\n",
    "data_dir = Path(f\"{CFG.data_dir}\")\n",
    "\n",
    "def load_pdf_text(path: Path) -> str:\n",
    "    reader = PdfReader(str(path))\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text() or \"\"\n",
    "        text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def build_documents(data_dir: Path) -> List[Document]:\n",
    "    docs = []\n",
    "    for path in data_dir.glob(\"*\"):\n",
    "        if path.suffix.lower() == \".pdf\":\n",
    "            text = load_pdf_text(path)\n",
    "        else:\n",
    "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "        if text.strip():\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": path.name}))\n",
    "    return docs\n",
    "\n",
    "def chunk_documents(docs: List[Document], chunk_size: int, overlap: int) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap, add_start_index=True)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    for i, d in enumerate(chunks):\n",
    "        d.metadata = {**d.metadata, \"chunk_id\": i}\n",
    "    return chunks\n",
    "\n",
    "docs = build_documents(data_dir)\n",
    "chunks = chunk_documents(docs, CFG.chunk_size, CFG.chunk_overlap)\n",
    "len(docs), len(chunks), [d.metadata for d in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189d92f",
   "metadata": {},
   "source": [
    "## Embedding és vektortár\n",
    "\n",
    "Itt a feldarabolt dokumentumokat embeddingekre alakítjuk, majd egy vektortárba (FAISS) kerülnek a hatékony kereséshez.\n",
    "\n",
    "- **`HuggingFaceEmbeddings`**: a `sentence-transformers/all-MiniLM-L6-v2` modell segítségével a szövegeket numerikus vektortérbe képezi le, ez lehetővé teszi, hogy a rendszer a tartalmi hasonlóság alapján keressen és találjon releváns szövegrészeket.\n",
    "- **`FAISS`**: lokálisan futó, nagy teljesítményű vektortár.\n",
    "- **`retriever`**: beállítja, hogy kereséskor hány releváns chunkot adjon vissza, ez lesz a fő komponens, amelyet a későbbi agent folyamat használ a tudás előhívására."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97b1491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def build_vectorstore(chunks: List[Document], embedding_model_name: str) -> FAISS:\n",
    "    embedder = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    vs = FAISS.from_documents(chunks, embedder)\n",
    "    return vs\n",
    "\n",
    "vectorstore = build_vectorstore(chunks, CFG.embedding_model_name)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": CFG.top_k})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f6e51",
   "metadata": {},
   "source": [
    "## LLM inicializálás\n",
    "\n",
    "Ebben betöltünk egy nyílt forrású LLM-et a HuggingFace `transformers` könyvtárából, majd egy egyszerű `generate` függvényt definiálunk.\n",
    "\n",
    "- **`make_llm`**  \n",
    "  - `AutoTokenizer` és `AutoModelForSeq2SeqLM` segítségével betölti a választott modellt\n",
    "  - `pipeline(\"text2text-generation\")` formátumban ad vissza egy használható generáló objektumot.  \n",
    "\n",
    "- **`generate`**  \n",
    "  - egyetlen szöveges promptból választ ad\n",
    "  - `max_new_tokens`: maximális válasz-hossz \n",
    "  - `temperature`: befolyásolja a determinisztikusságot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8959175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "def make_llm(model_name: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    pipe = pipeline(\"text2text-generation\", model=mdl, tokenizer=tok)\n",
    "    return pipe\n",
    "\n",
    "llm = make_llm(CFG.llm_model_name)\n",
    "\n",
    "def generate(prompt: str, max_new_tokens: int = 256, temperature: float = CFG.temperature) -> str:\n",
    "    out = llm(prompt, max_new_tokens=max_new_tokens, do_sample=temperature>0.0, temperature=temperature)[0][\"generated_text\"]\n",
    "    return out.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f4681",
   "metadata": {},
   "source": [
    "## RAG keresés és kontextus összeállítás\n",
    "\n",
    "Ebben a lépésben a retriever segítségével releváns dokumentumokat keresünk, majd azokból összeállítunk egy kontextus-blokkot, amelyet a nyelvi modellnek adunk át.\n",
    "\n",
    "- **`rag_search`**\n",
    "  - A felhasználói lekérdezést lefuttatja a vektortárban.\n",
    "  - Visszaad egy listát a legrelevánsabb `Document` objektumokból.\n",
    "\n",
    "- **`build_context`**\n",
    "  - A talált dokumentumokat formázott szövegblokká alakítja.\n",
    "  - Tartalmazza:\n",
    "    - sorszámot (`[i]`),\n",
    "    - forrásfájlt (`src`),\n",
    "    - chunk azonosítót (`chunk`)\n",
    "  - Ez biztosítja, hogy az LLM lássa a szöveget és az eredetére vonatkozó metaadatokat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58ddc155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_search(query: str) -> List[Document]:\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "def build_context(docs: List[Document]) -> str:\n",
    "    blocks = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        src = d.metadata.get(\"source\", \"unknown\")\n",
    "        cid = d.metadata.get(\"chunk_id\", i)\n",
    "        blocks.append(f\"[{i}] (src={src}, chunk={cid})\\n{d.page_content}\")\n",
    "    return \"\\n\\n\".join(blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049d588",
   "metadata": {},
   "source": [
    "## Agent workflow\n",
    "\n",
    "Itt hozzuk létre a LangGraph alapú agentet.\n",
    "\n",
    "### Állapot (`AgentState`)\n",
    "Az agent állapotát egy `TypedDict` írja le, ez tartalmazza többek között:\n",
    "- **`question`** – a felhasználó kérdése  \n",
    "- **`plan`**, **`step`** – tervezés és lépéskövetés  \n",
    "- **`retrieved`**, **`context`** – talált dokumentumok és a kontextus  \n",
    "- **`answer`**, **`citations`** – generált válasz és hivatkozások  \n",
    "- **`iterations`**, **`score`**, **`logs`** – önreflexió és naplózás  \n",
    "\n",
    "### Fő csomópontok\n",
    "- **`planner_node`**: alapértelmezett tervet ad (search → synthesize → reflect).  \n",
    "- **`search_node`**: keresést futtat, kontextust épít.  \n",
    "- **`synthesize_node`**: LLM-mel választ generál, hivatkozásokkal.  \n",
    "- **`reflect_node`**: pontozza a választ egyszerű logika alapján (forráshivatkozások és hossz).  \n",
    "\n",
    "### Vezérlés\n",
    "- **`controller`**: ha az értékelés < 0.55 és még nem érte el a max iterációt, újrakérdez → vissza a *search* lépéshez, különben az agent lezárja a folyamatot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5622e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    plan: List[str]\n",
    "    step: int\n",
    "    retrieved: List[Document]\n",
    "    context: str\n",
    "    answer: str\n",
    "    citations: List[Dict[str, Any]]\n",
    "    iterations: int\n",
    "    logs: List[str]\n",
    "    score: float\n",
    "\n",
    "ANSWER_PROMPT = \"\"\"You are a helpful assistant.\n",
    "Read the context and answer the question in 1–2 complete sentences.\n",
    "Question: {question}\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "def planner_node(state: AgentState) -> AgentState:\n",
    "    if not state.get(\"plan\"):\n",
    "        state[\"plan\"] = [\"search\", \"synthesize\", \"reflect\"]\n",
    "        state[\"step\"] = 0\n",
    "        state.setdefault(\"iterations\", 0)\n",
    "        state.setdefault(\"logs\", []).append(\"Planner: plan: search → synthesize → reflect\")\n",
    "    return state\n",
    "\n",
    "def search_node(state: AgentState) -> AgentState:\n",
    "    q = state[\"question\"]\n",
    "    docs = rag_search(q)\n",
    "    state[\"retrieved\"] = docs\n",
    "    state[\"context\"] = build_context(docs)\n",
    "    state.setdefault(\"logs\", []).append(f\"Search: {len(docs)} relevant snippets\")\n",
    "    return state\n",
    "\n",
    "def synthesize_node(state: AgentState) -> AgentState:\n",
    "    prompt = ANSWER_PROMPT.format(question=state[\"question\"], context=state.get(\"context\", \"\"))\n",
    "    ans = generate(prompt, max_new_tokens=320)\n",
    "    citations = []\n",
    "    for i, d in enumerate(state.get(\"retrieved\", []), 1):\n",
    "        citations.append({\"ref\": i, \"source\": d.metadata.get(\"source\", \"unknown\"), \"chunk_id\": d.metadata.get(\"chunk_id\")})\n",
    "    state[\"answer\"] = ans\n",
    "    state[\"citations\"] = citations\n",
    "    state.setdefault(\"logs\", []).append(\"Synthesize: answer generated\")\n",
    "    return state\n",
    "\n",
    "def reflect_score(state: AgentState) -> float:\n",
    "    ans = state.get(\"answer\", \"\")\n",
    "    srcs = len(state.get(\"citations\", []))\n",
    "    length_bonus = min(len(ans) / 400, 1.0)\n",
    "    score = 0.5 * (srcs > 0) + 0.5 * length_bonus\n",
    "    return float(score)\n",
    "\n",
    "def reflect_node(state: AgentState) -> AgentState:\n",
    "    score = reflect_score(state)\n",
    "    state[\"score\"] = score\n",
    "    state[\"iterations\"] = state.get(\"iterations\", 0) + 1\n",
    "    state.setdefault(\"logs\", []).append(f\"Reflect: score={score:.2f}, iter={state['iterations']}\")\n",
    "    return state\n",
    "\n",
    "def controller(state: AgentState) -> str:\n",
    "    if state.get(\"score\", 0) < 0.55 and state.get(\"iterations\", 0) < CFG.max_iterations:\n",
    "        refined = state[\"question\"] + \" Please explain more deeply and give a definition.\"\n",
    "        state[\"question\"] = refined\n",
    "        state.setdefault(\"logs\", []).append(\"Controller: low score → another round of search and synthesis\")\n",
    "        return \"search\"\n",
    "    else:\n",
    "        state.setdefault(\"logs\", []).append(\"Controller: adequate score or limit reached → END\")\n",
    "        return \"end\"\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"search\", search_node)\n",
    "workflow.add_node(\"synthesize\", synthesize_node)\n",
    "workflow.add_node(\"reflect\", reflect_node)\n",
    "\n",
    "workflow.set_entry_point(\"planner\")\n",
    "workflow.add_edge(\"planner\", \"search\")\n",
    "workflow.add_edge(\"search\", \"synthesize\")\n",
    "workflow.add_edge(\"synthesize\", \"reflect\")\n",
    "workflow.add_conditional_edges(\"reflect\", controller, {\"search\": \"search\", \"end\": END})\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a4aa9",
   "metadata": {},
   "source": [
    "## Demó futtatás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9e51d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KÉRDÉS: What does embedding mean?\n",
      "\n",
      "VÁLASZ:\n",
      " the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048\n",
      "\n",
      "HIVATKOZÁSOK: [{'ref': 1, 'source': 'attention_is_all_you_need.pdf', 'chunk_id': 33}, {'ref': 2, 'source': 'attention_is_all_you_need.pdf', 'chunk_id': 35}, {'ref': 3, 'source': 'attention_is_all_you_need.pdf', 'chunk_id': 50}, {'ref': 4, 'source': 'attention_is_all_you_need.pdf', 'chunk_id': 31}]\n",
      "\n",
      "NAPLÓ:\n",
      " - Planner: plan: search → synthesize → reflect\n",
      " - Search: 4 relevant snippets\n",
      " - Synthesize: answer generated\n",
      " - Reflect: score=0.83, iter=1\n",
      " - Controller: adequate score or limit reached → END\n",
      "KÉRDÉS: What is the essence of attention?\n",
      "\n",
      "VÁLASZ:\n",
      " relating different positions of a single sequence in order to compute a representation of the sequence\n",
      "\n",
      "HIVATKOZÁSOK: [{'ref': 1, 'source': 'attention_is_all_you_need.pdf', 'chunk_id': 44}, {'ref': 2, 'source': 'attention_is_all_you_need.pdf', 'chunk_id': 12}, {'ref': 3, 'source': 'attention_is_all_you_need.pdf', 'chunk_id': 84}, {'ref': 4, 'source': 'attention_is_all_you_need.pdf', 'chunk_id': 17}]\n",
      "\n",
      "NAPLÓ:\n",
      " - Planner: plan: search → synthesize → reflect\n",
      " - Search: 4 relevant snippets\n",
      " - Synthesize: answer generated\n",
      " - Reflect: score=0.63, iter=1\n",
      " - Controller: adequate score or limit reached → END\n",
      "KÉRDÉS: What can lithium be used for?\n",
      "\n",
      "VÁLASZ:\n",
      " a useful ingredient in ovenproof ceramics and in the glass-ceramic surfaces on induction cooktops. It is employed as an absorption material in air conditioners and humidity control systems. The lithium-6 isotope is used in the core of nuclear weapons, its reactivity escalating the power of a thermonuclear explosion. It’s in fireworks too, burning a brilliant red. [1] (src=The Secret, Magical Life Of Lithium - NOEMA.pdf, chunk=144) https://www.noemamag.com/the-secret-magical-life-of-lithium/ 11/20 Because it lowers thermal expansion, lithium is a useful ingredient in ovenproof ceramics and in the glass-ceramic surfaces on induction cooktops. It is employed as an absorption material in air conditioners and humidity control systems. The lithium-6 isotope is used in the core of nuclear weapons, its reactivity escalating the power of a thermonuclear explosion. It’s in fireworks too, burning a brilliant red. [2] (src=The Secret, Magical Life Of Lithium - NOEMA.pdf, chunk=144) containing lithium salts was adopted for use in aircraft engines; today, it dominates nearly three-quarters of lubricating needs across all industries. It is a valuable material in metall\n",
      "\n",
      "HIVATKOZÁSOK: [{'ref': 1, 'source': 'The Secret, Magical Life Of Lithium - NOEMA.pdf', 'chunk_id': 145}, {'ref': 2, 'source': 'The Secret, Magical Life Of Lithium - NOEMA.pdf', 'chunk_id': 144}, {'ref': 3, 'source': 'The Secret, Magical Life Of Lithium - NOEMA.pdf', 'chunk_id': 133}, {'ref': 4, 'source': 'The Secret, Magical Life Of Lithium - NOEMA.pdf', 'chunk_id': 113}]\n",
      "\n",
      "NAPLÓ:\n",
      " - Planner: plan: search → synthesize → reflect\n",
      " - Search: 4 relevant snippets\n",
      " - Synthesize: answer generated\n",
      " - Reflect: score=1.00, iter=1\n",
      " - Controller: adequate score or limit reached → END\n",
      "KÉRDÉS: Who invented the battery and when?\n",
      "\n",
      "VÁLASZ:\n",
      " In 1859, the French physicist Gaston Planté devised a rechargeable battery using lead electrodes in sulfuric acid, a basic formula that, with some improvements, is still used to start cars, boats, lawnmowers and other engines.\n",
      "\n",
      "HIVATKOZÁSOK: [{'ref': 1, 'source': 'The Secret, Magical Life Of Lithium - NOEMA.pdf', 'chunk_id': 140}, {'ref': 2, 'source': 'The Secret, Magical Life Of Lithium - NOEMA.pdf', 'chunk_id': 147}, {'ref': 3, 'source': 'The Secret, Magical Life Of Lithium - NOEMA.pdf', 'chunk_id': 139}, {'ref': 4, 'source': 'The Secret, Magical Life Of Lithium - NOEMA.pdf', 'chunk_id': 157}]\n",
      "\n",
      "NAPLÓ:\n",
      " - Planner: plan: search → synthesize → reflect\n",
      " - Search: 4 relevant snippets\n",
      " - Synthesize: answer generated\n",
      " - Reflect: score=0.78, iter=1\n",
      " - Controller: adequate score or limit reached → END\n",
      "KÉRDÉS: What has the most impact on climate change?\n",
      "\n",
      "VÁLASZ:\n",
      " The key technological revolution with respect to climate change is the one that powered industrialisation. Capitalists discovered, refined, and systematically exploited the ability to convert fossil fuels into work, allowing them to add vast amounts of human/animal labour equivalents to the economy\n",
      "\n",
      "HIVATKOZÁSOK: [{'ref': 1, 'source': 'We should fix climate change — but we should not regret it - ABC Religion & Ethics.pdf', 'chunk_id': 197}, {'ref': 2, 'source': 'We should fix climate change — but we should not regret it - ABC Religion & Ethics.pdf', 'chunk_id': 189}, {'ref': 3, 'source': 'We should fix climate change — but we should not regret it - ABC Religion & Ethics.pdf', 'chunk_id': 184}, {'ref': 4, 'source': 'We should fix climate change — but we should not regret it - ABC Religion & Ethics.pdf', 'chunk_id': 209}]\n",
      "\n",
      "NAPLÓ:\n",
      " - Planner: plan: search → synthesize → reflect\n",
      " - Search: 4 relevant snippets\n",
      " - Synthesize: answer generated\n",
      " - Reflect: score=0.87, iter=1\n",
      " - Controller: adequate score or limit reached → END\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_agent(question: str) -> Dict[str, Any]:\n",
    "    state: AgentState = {\"question\": question}\n",
    "    result = app.invoke(state)\n",
    "    return result\n",
    "\n",
    "demo_questions = [\n",
    "    \"What does embedding mean?\",\n",
    "    \"What is the essence of attention?\",\n",
    "    \"What can lithium be used for?\",\n",
    "    \"Who invented the battery and when?\",\n",
    "    \"What has the most impact on climate change?\",\n",
    "]\n",
    "\n",
    "for q in demo_questions:\n",
    "    out = run_agent(q)\n",
    "    print(\"KÉRDÉS:\", q)\n",
    "    print(\"\\nVÁLASZ:\\n\", out.get(\"answer\", \"\"))\n",
    "    print(\"\\nHIVATKOZÁSOK:\", out.get(\"citations\", []))\n",
    "    print(\"\\nNAPLÓ:\")\n",
    "    for log in out.get(\"logs\", []):\n",
    "        print(\" -\", log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63a3c0",
   "metadata": {},
   "source": [
    "## Bottleneckek, teljesítménymérési és továbbfejlesztési ötletek\n",
    "\n",
    "### Jelenlegi bottleneckek\n",
    "- **LLM**: a `flan-t5-base` modell nem feltétlen ad megfelelő válaszokat, egy nagyobb modellel jobb eredményeket lehetne elérni\n",
    "- **Chunkolás**: fix méretű chunkolásnál előfordulhat, hogy fontos információ szétszakad vagy kimarad.\n",
    "- **Egyszerű minőségértékelés**: a `reflect_score` csak hosszt és forráshivatkozást figyel, lehetne pontosabban értékelni\n",
    "- **Agentic loop**: az újra futás nem feltétlen javít az eredményen\n",
    "- **Agent tervezés**: a `planner_node` mindig ugyanazt az alapértelmezett tervet adja. Nincs valódi adaptivitás a felhasználói kérdéshez vagy a keresés eredményeihez.\n",
    "\n",
    "### Teljesítménymérési ötletek\n",
    "- **Retrieval hatékonyság**: Megállapítani, hogy a releváns találatok hanyad része kerül be a top-k listába.  \n",
    "- **Generált válaszok**: LLM‑alapú minőségellenőrzés\n",
    "- **Latency**: Keresés és generálás időmérése külön-külön.  \n",
    "\n",
    "### Továbbfejlesztési ötletek\n",
    "- **LLM**: jobb modell a pontosabb válaszokhoz\n",
    "- **Dinamikus chunkolás**: bekezdés- vagy mondat-alapú darabolás a fix karakterhossz helyett\n",
    "- **Fejlettebb értékelés**: LLM-alapú reflexió, logikai következetesség ellenőrzése\n",
    "- **Bővített agent logika**: több eszköz, kifinomultabb vezérlés"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
